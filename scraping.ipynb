{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777afeb6",
   "metadata": {},
   "source": [
    "## This file will focus strictly on scraping data from the website fbref.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c505bcaf-a286-4bff-9a2f-42650bafdad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (0.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\haiaj\\appdata\\roaming\\python\\python311\\site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: Levenshtein==0.27.1 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from python-Levenshtein->-r requirements.txt (line 5)) (0.27.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\haiaj\\anaconda3\\lib\\site-packages (from Levenshtein==0.27.1->python-Levenshtein->-r requirements.txt (line 5)) (3.13.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\haiaj\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c64d049-ea7f-4be3-b8e0-02ec7829e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06bf7b",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "\n",
    "`get_data_from_txt()` takes a file path, reads the HTML content, and returns a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74caa9be-6fb2-4199-a18b-44b7a9dcdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_txt(file_path):\n",
    "    # Read the HTML content from the file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the table\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if not table:\n",
    "        raise ValueError(\"No table found in the HTML content\")\n",
    "    \n",
    "    # Extract column names from thead\n",
    "    thead = table.find('thead')\n",
    "    if thead:\n",
    "        column_headers = thead.find_all('th')\n",
    "        column_names = [th.get('aria-label', th.text.strip()) for th in column_headers]\n",
    "    else:\n",
    "        column_names = []\n",
    "    \n",
    "    # Extract data from tbody\n",
    "    tbody = table.find('tbody')\n",
    "    data = []\n",
    "    if tbody:\n",
    "        for row in tbody.find_all('tr'):\n",
    "            row_data = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
    "            data.append(row_data)\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Assign column names, truncating or padding as necessary\n",
    "    if len(df.columns) > len(column_names):\n",
    "        # If there are more columns in the data than names, use the first len(column_names) columns\n",
    "        df = df.iloc[:, :len(column_names)]\n",
    "    elif len(df.columns) < len(column_names):\n",
    "        # If there are fewer columns in the data than names, truncate the column names\n",
    "        column_names = column_names[:len(df.columns)]\n",
    "    \n",
    "    df.columns = column_names\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d08eda",
   "metadata": {},
   "source": [
    "### Getting Team data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbf928",
   "metadata": {},
   "source": [
    "`get_squad_stats()` scrapes fbref link html for standard squad stats, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f825cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad_stats():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "\n",
    "    # ðŸ§¢ Pretend to be a real Chrome browser\n",
    "    headers = {\n",
    "        'User-Agent': (\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "            'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "            'Chrome/124.0.0.0 Safari/537.36'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/stats/squads/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2022-2023\"]  # Start small to test\n",
    "\n",
    "    for season in seasons:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Raise error for 403s or other HTTP issues\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table_div = soup.find('div', id='div_stats_teams_standard_for')\n",
    "            \n",
    "            if table_div:\n",
    "                with open(f'data_html/squad_stats_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved squad stats for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            time.sleep(random.uniform(5, 10))  # ðŸ’¤ Nap time\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "            \n",
    "    print(\"âœ… All seasons processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60713192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/stats/squads/2022-2023-Big-5-European-Leagues-Stats\n",
      "âœ… All seasons processed\n"
     ]
    }
   ],
   "source": [
    "get_squad_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743528c",
   "metadata": {},
   "source": [
    "`get_squad_wages()` scrapes fbref link html for squad wages, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ae2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad_wages():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/wages/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "    \n",
    "    for season in seasons:\n",
    "        #if season == \"2023-2024\":\n",
    "           # url = \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\"\n",
    "       # else:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the div containing the table\n",
    "            table_div = soup.find('div', id='div_squad_wages')\n",
    "            \n",
    "            if table_div:\n",
    "                # Save the HTML content of the div to a file\n",
    "                with open(f'data_html/squad_wages_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved squad wages for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            # Introduce a random delay between 5 and 10 seconds\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "\n",
    "    print(\"All seasons processed\")\n",
    "    return f'data_html/squad_wages_{season}.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2325a6",
   "metadata": {},
   "source": [
    "### Getting Player data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a1a6b",
   "metadata": {},
   "source": [
    "`get_standard_stats()` scrapes fbref link html for defensive stats, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18b414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_stats():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/stats/players/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "    \n",
    "    for season in seasons:\n",
    "        #if season == \"2023-2024\":\n",
    "            #url = \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\"\n",
    "        #else:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the div containing the table\n",
    "            table_div = soup.find('div', id='div_stats_standard')\n",
    "            \n",
    "            if table_div:\n",
    "                # Save the HTML content of the div to a file\n",
    "                with open(f'data_html/standard_stats_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved standard stats for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            # Introduce a random delay between 5 and 10 seconds\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "\n",
    "    print(\"All seasons processed\")\n",
    "    return f'data_html/standard_stats_{season}.txt'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516e41e",
   "metadata": {},
   "source": [
    "`get_defensive_stats()` scrapes fbref link html for defensive stats, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "238f043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defensive_stats():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/defense/players/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "    \n",
    "    for season in seasons:\n",
    "        #if season == \"2023-2024\":\n",
    "           # url = \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\"\n",
    "       # /else:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the div containing the table\n",
    "            table_div = soup.find('div', id='div_stats_defense')\n",
    "            \n",
    "            if table_div:\n",
    "                # Save the HTML content of the div to a file\n",
    "                with open(f'data_html/defensive_stats_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved defensive stats for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            # Introduce a random delay between 5 and 10 seconds\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "\n",
    "    print(\"All seasons processed\")\n",
    "    return f'data_html/defensive_stats_{season}.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81790d",
   "metadata": {},
   "source": [
    "`get_passing_stats()` scrapes fbref link html for defensive stats, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6632504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passing_stats():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/passing/players/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "    \n",
    "    for season in seasons:\n",
    "        #if season == \"2023-2024\":\n",
    "           # url = \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\"\n",
    "       # else:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the div containing the table\n",
    "            table_div = soup.find('div', id='div_stats_passing')\n",
    "            \n",
    "            if table_div:\n",
    "                # Save the HTML content of the div to a file\n",
    "                with open(f'data_html/passing_stats_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved passing stats for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            # Introduce a random delay between 5 and 10 seconds\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "\n",
    "    print(\"All seasons processed\")\n",
    "    return f'data_html/passing_stats_{season}.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://fbref.com/en/comps/Big5/2017-2018/shooting/players/2017-2018-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2017-2018\n",
      "Fetching: https://fbref.com/en/comps/Big5/2018-2019/shooting/players/2018-2019-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2018-2019\n",
      "Fetching: https://fbref.com/en/comps/Big5/2019-2020/shooting/players/2019-2020-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2019-2020\n",
      "Fetching: https://fbref.com/en/comps/Big5/2020-2021/shooting/players/2020-2021-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2020-2021\n",
      "Fetching: https://fbref.com/en/comps/Big5/2021-2022/shooting/players/2021-2022-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2021-2022\n",
      "Fetching: https://fbref.com/en/comps/Big5/2022-2023/shooting/players/2022-2023-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2022-2023\n",
      "Fetching: https://fbref.com/en/comps/Big5/2023-2024/shooting/players/2023-2024-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2023-2024\n",
      "All requested seasons processed.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_shooting_stats(seasons=None):\n",
    "    if seasons is None:\n",
    "        seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode (no browser UI)\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    os.makedirs(\"data_html\", exist_ok=True)\n",
    "\n",
    "    for season in seasons:\n",
    "        url = f\"https://fbref.com/en/comps/Big5/{season}/shooting/players/{season}-Big-5-European-Leagues-Stats\"\n",
    "        print(f\"Fetching: {url}\")\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            # Wait up to 20 seconds for the table div to appear\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.ID, \"div_stats_shooting\"))\n",
    "            )\n",
    "            table_div = driver.find_element(By.ID, \"div_stats_shooting\")\n",
    "            html = table_div.get_attribute('outerHTML')\n",
    "            with open(f\"data_html/shooting_stats_{season}_selenium.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(html)\n",
    "            print(f\"Saved shooting stats for {season}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not get table for {season}: {e}\")\n",
    "        # Space out requests to avoid being flagged\n",
    "        time.sleep(8)\n",
    "\n",
    "    driver.quit()\n",
    "    print(\"All requested seasons processed.\")\n",
    "\n",
    "# Example usage: only run this for shooting stats\n",
    "#get_shooting_stats()  # Or add more seasons as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b697c52",
   "metadata": {},
   "source": [
    "`get_goalkeeping_stats()` scrapes fbref link html for goalkeeper stats, puts it in txt file, and returns txt file address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ad3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goalkeeping_stats():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('https://', adapter)\n",
    "    session.mount('http://', adapter)\n",
    "    \n",
    "    base_url = \"https://fbref.com/en/comps/Big5/{}/keepers/players/{}-Big-5-European-Leagues-Stats\"\n",
    "    seasons = [\"2017-2018\", \"2018-2019\", \"2019-2020\", \"2020-2021\", \"2021-2022\", \"2022-2023\", \"2023-2024\"]\n",
    "    \n",
    "    for season in seasons:\n",
    "        #if season == \"2023-2024\":\n",
    "           # url = \"https://fbref.com/en/comps/Big5/defense/players/Big-5-European-Leagues-Stats\"\n",
    "       # else:\n",
    "        url = base_url.format(season, season)\n",
    "        \n",
    "        try:\n",
    "            response = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find the div containing the table\n",
    "            table_div = soup.find('div', id='div_stats_keeper')\n",
    "            \n",
    "            if table_div:\n",
    "                # Save the HTML content of the div to a file\n",
    "                with open(f'data_html/goalkeeping_stats_{season}.txt', 'w', encoding='utf-8') as file:\n",
    "                    file.write(str(table_div))\n",
    "                print(f\"Saved goalkeeping stats for {season}\")\n",
    "            else:\n",
    "                print(f\"Table div not found for {season}\")\n",
    "            \n",
    "            # Introduce a random delay between 5 and 10 seconds\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to fetch data for season {season}: {e}\")\n",
    "\n",
    "    print(\"All seasons processed\")\n",
    "    return f'data_html/goalkeeping_stats_{season}.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a0231",
   "metadata": {},
   "source": [
    "### Get all data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943f121",
   "metadata": {},
   "source": [
    "`get_all_seasons()` loops through all the txt files, extracts the data, and adds it to a dataframe for all seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eda85a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_seasons():\n",
    "    # List of seasons to loop through\n",
    "    seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n",
    "    # dataframes to store squad stats, and wages\n",
    "    seasons_squads_stats = []\n",
    "    seasons_shooting_stats = []\n",
    "    seasons_squads_wages = []\n",
    "    seasons_standard_stats = []\n",
    "    seasons_defensive_stats = []\n",
    "    seasons_passing_stats = []\n",
    "    seasons_goalkeeping_stats = []\n",
    "    \n",
    "    # Getting squads stats data\n",
    "    get_squad_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/squad_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_squads_stats.append(df)\n",
    "        \n",
    "    # Getting squad wages data\n",
    "    get_squad_wages()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/squad_wages_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_squads_wages.append(df)\n",
    "    \n",
    "    # Getting standard player stats\n",
    "    get_standard_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/standard_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_standard_stats.append(df)\n",
    "        \n",
    "    # Getting defensive player stats\n",
    "    get_defensive_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/defensive_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_defensive_stats.append(df)\n",
    "    \n",
    "    # Getting passing player stats\n",
    "    get_passing_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/passing_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_passing_stats.append(df)\n",
    "        \n",
    "    get_shooting_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/shooting_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_shooting_stats.append(df)\n",
    "        \n",
    "    # Getting goalkeeping player stats\n",
    "    get_goalkeeping_stats()\n",
    "    for season in seasons:\n",
    "        file_path = f'data_html/goalkeeping_stats_{season}.txt'\n",
    "        df = get_data_from_txt(file_path)\n",
    "        df[\"Season\"] = season\n",
    "        seasons_goalkeeping_stats.append(df)\n",
    "    \n",
    "    \n",
    "    squads_stats = pd.concat(seasons_squads_stats, ignore_index=True)\n",
    "    squads_wages = pd.concat(seasons_squads_wages, ignore_index=True)\n",
    "    standard_stats = pd.concat(seasons_standard_stats, ignore_index=True)\n",
    "    defensive_stats = pd.concat(seasons_defensive_stats, ignore_index=True)\n",
    "    passing_stats = pd.concat(seasons_passing_stats, ignore_index=True)\n",
    "    shooting_stats = pd.concat(seasons_shooting_stats, ignore_index=True)\n",
    "    goalkeeping_stats = pd.concat(seasons_goalkeeping_stats, ignore_index=True)\n",
    "    \n",
    "    return squads_stats, squads_wages, standard_stats, defensive_stats, passing_stats, shooting_stats, goalkeeping_stats\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "834c7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_data():\n",
    "    # get squads stats, squad wages, standard player stats, defensive stats, passing stats, and goalkeeping for all seasons\n",
    "    squads_stats, squads_wages, standard_stats, defensive_stats, passing_stats, shooting_stats, goalkeeping_stats = get_all_seasons()\n",
    "\n",
    "    # output paths\n",
    "    stats_output_path = 'uncleaned_data_csv\\seasons_stats.csv'\n",
    "    wages_output_path = 'uncleaned_data_csv\\seasons_wages.csv'\n",
    "    standard_output_path = 'uncleaned_data_csv\\standard.csv'\n",
    "    defending_output_path = 'uncleaned_data_csv\\defending.csv'\n",
    "    shooting_output_path = 'uncleaned_data_csv\\shooting.csv'\n",
    "    passing_output_path = 'uncleaned_data_csv\\passing.csv'\n",
    "    \n",
    "    # To csv\n",
    "    squads_stats.to_csv(stats_output_path, index=False)\n",
    "    squads_wages.to_csv(wages_output_path, index=False)\n",
    "    standard_stats.to_csv(standard_output_path, index=False)\n",
    "    defensive_stats.to_csv(defending_output_path, index=False)\n",
    "    passing_stats.to_csv(passing_output_path, index=False)\n",
    "    shooting_stats.to_csv(shooting_output_path, index=False)\n",
    "    \n",
    "    # print them out\n",
    "    print(squads_stats.head())\n",
    "    print(squads_wages.head())\n",
    "    print(standard_stats.head())\n",
    "    print(shooting_stats.head())\n",
    "    print(defensive_stats.head())\n",
    "    print(passing_stats.head())\n",
    "    print(goalkeeping_stats.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b68f20d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/stats/squads/2022-2023-Big-5-European-Leagues-Stats\n",
      "âœ… All seasons processed\n",
      "Failed to fetch data for season 2017-2018: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2017-2018/wages/2017-2018-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2018-2019: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2018-2019/wages/2018-2019-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2019-2020: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2019-2020/wages/2019-2020-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2020-2021: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2020-2021/wages/2020-2021-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2021-2022: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2021-2022/wages/2021-2022-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/wages/2022-2023-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2023-2024: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2023-2024/wages/2023-2024-Big-5-European-Leagues-Stats\n",
      "All seasons processed\n",
      "Failed to fetch data for season 2017-2018: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2017-2018/stats/players/2017-2018-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2018-2019: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2018-2019/stats/players/2018-2019-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2019-2020: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2019-2020/stats/players/2019-2020-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2020-2021: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2020-2021/stats/players/2020-2021-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2021-2022: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2021-2022/stats/players/2021-2022-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/stats/players/2022-2023-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2023-2024: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2023-2024/stats/players/2023-2024-Big-5-European-Leagues-Stats\n",
      "All seasons processed\n",
      "Failed to fetch data for season 2017-2018: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2017-2018/defense/players/2017-2018-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2018-2019: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2018-2019/defense/players/2018-2019-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2019-2020: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2019-2020/defense/players/2019-2020-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2020-2021: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2020-2021/defense/players/2020-2021-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2021-2022: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2021-2022/defense/players/2021-2022-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/defense/players/2022-2023-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2023-2024: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2023-2024/defense/players/2023-2024-Big-5-European-Leagues-Stats\n",
      "All seasons processed\n",
      "Failed to fetch data for season 2017-2018: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2017-2018/passing/players/2017-2018-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2018-2019: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2018-2019/passing/players/2018-2019-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2019-2020: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2019-2020/passing/players/2019-2020-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2020-2021: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2020-2021/passing/players/2020-2021-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2021-2022: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2021-2022/passing/players/2021-2022-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2022-2023: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2022-2023/passing/players/2022-2023-Big-5-European-Leagues-Stats\n",
      "Failed to fetch data for season 2023-2024: 403 Client Error: Forbidden for url: https://fbref.com/en/comps/Big5/2023-2024/passing/players/2023-2024-Big-5-European-Leagues-Stats\n",
      "All seasons processed\n",
      "Fetching: https://fbref.com/en/comps/Big5/2017-2018/shooting/players/2017-2018-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2017-2018\n",
      "Fetching: https://fbref.com/en/comps/Big5/2018-2019/shooting/players/2018-2019-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2018-2019\n",
      "Fetching: https://fbref.com/en/comps/Big5/2019-2020/shooting/players/2019-2020-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2019-2020\n",
      "Fetching: https://fbref.com/en/comps/Big5/2020-2021/shooting/players/2020-2021-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2020-2021\n",
      "Fetching: https://fbref.com/en/comps/Big5/2021-2022/shooting/players/2021-2022-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2021-2022\n",
      "Fetching: https://fbref.com/en/comps/Big5/2022-2023/shooting/players/2022-2023-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2022-2023\n",
      "Fetching: https://fbref.com/en/comps/Big5/2023-2024/shooting/players/2023-2024-Big-5-European-Leagues-Stats\n",
      "Saved shooting stats for 2023-2024\n",
      "All requested seasons processed.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_html/shooting_stats_2017-2018.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m, in \u001b[0;36mscrape_all_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_all_data\u001b[39m():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# get squads stats, squad wages, standard player stats, defensive stats, passing stats, and goalkeeping for all seasons\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     squads_stats, squads_wages, standard_stats, defensive_stats, passing_stats, shooting_stats, goalkeeping_stats \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_seasons\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# output paths\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     stats_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muncleaned_data_csv\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mseasons_stats.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 56\u001b[0m, in \u001b[0;36mget_all_seasons\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m season \u001b[38;5;129;01min\u001b[39;00m seasons:\n\u001b[0;32m     55\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_html/shooting_stats_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_data_from_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m season\n\u001b[0;32m     58\u001b[0m     seasons_shooting_stats\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mget_data_from_txt\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_from_txt\u001b[39m(file_path):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Read the HTML content from the file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m         html_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_html/shooting_stats_2017-2018.txt'"
     ]
    }
   ],
   "source": [
    "scrape_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c54b5d",
   "metadata": {},
   "source": [
    "#Sanity Check#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season\n",
      "2021-2022    3036\n",
      "2022-2023    3004\n",
      "2023-2024    2966\n",
      "2020-2021    2934\n",
      "2019-2020    2841\n",
      "2017-2018    2799\n",
      "2018-2019    2762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('uncleaned_data_csv/defending.csv')\n",
    "print(df['Season'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22120bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Season\n",
       "2017-2018    2799\n",
       "2018-2019    2762\n",
       "2019-2020    2841\n",
       "2020-2021    2934\n",
       "2021-2022    3036\n",
       "2022-2023    3004\n",
       "2023-2024    2966\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('uncleaned_data_csv/passing.csv')\n",
    "df.groupby('Season').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93593f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Progressive Passing Distance    0.057025\n",
       "Passes Attempted                0.022417\n",
       "Year of birth                   0.015289\n",
       "Nation                          0.006243\n",
       "Unnamed: 2                      0.000442\n",
       "90s Played                      0.000295\n",
       "Age                             0.000295\n",
       "Pass Completion % (Short)       0.000295\n",
       "Passes Attempted (Short)        0.000295\n",
       "Pass Completion % (Medium)      0.000295\n",
       "Total Passing Distance          0.000295\n",
       "Pass Completion %               0.000295\n",
       "Passes Completed                0.000295\n",
       "Passes Completed (Long)         0.000295\n",
       "Squad                           0.000295\n",
       "Competition                     0.000295\n",
       "Passes Attempted (Medium)       0.000295\n",
       "Position                        0.000295\n",
       "Player                          0.000295\n",
       "Rk                              0.000295\n",
       "Passes Attempted (Long)         0.000295\n",
       "Pass Completion % (Long)        0.000295\n",
       "Passes Completed (Medium)       0.000295\n",
       "Unnamed: 7                      0.000246\n",
       "Unnamed: 6                      0.000246\n",
       "Unnamed: 3                      0.000049\n",
       "Assists                         0.000000\n",
       "Unnamed: 0                      0.000000\n",
       "Passes Completed (Short)        0.000000\n",
       "Unnamed: 1                      0.000000\n",
       "Unnamed: 8                      0.000000\n",
       "Unnamed: 5                      0.000000\n",
       "Unnamed: 4                      0.000000\n",
       "Season                          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc2427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
